{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Dependency imports\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import matplotlib\n",
    "\n",
    "from datetime import datetime\n",
    "#from time import time\n",
    "#from tensorboard.python.keras.callbacks import TensorBoard\n",
    "\n",
    "from matplotlib import figure  # pylint: disable=g-import-not-at-top\n",
    "from matplotlib.backends import backend_agg\n",
    "import numpy as np\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from utils import gpu_session\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "# TODO(b/78137893): Integration tests currently fail with seaborn imports.\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "try:\n",
    "    import seaborn as sns  # pylint: disable=g-import-not-at-top\n",
    "\n",
    "    HAS_SEABORN = True\n",
    "except ImportError:\n",
    "    HAS_SEABORN = False\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "\n",
    "model_type = \"dense_layer\"\n",
    "IMAGE_SHAPE = [28, 28, 1]\n",
    "NUM_TRAIN_EXAMPLES = 60000  # 60000\n",
    "NUM_HELDOUT_EXAMPLES = 10000  # 10000\n",
    "NUM_CLASSES = 10\n",
    "NUM_GROUPS = 3\n",
    "# Distribution of digits to groups\n",
    "LABELS_CHANGE_DICT_GROUPED = {0: 0, 3: 0, 6: 0, 8: 0,  # 0\n",
    "                              2: 1, 5: 1,  # 1\n",
    "                              1: 2, 4: 2, 7: 2, 9: 2}  # 2\n",
    "LABELS_CHANGE_GROUPED = []\n",
    "for i in range(NUM_CLASSES):\n",
    "    LABELS_CHANGE_GROUPED.append(LABELS_CHANGE_DICT_GROUPED[i])\n",
    "LABELS_CHANGE_GROUPED = tuple(LABELS_CHANGE_GROUPED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(68.87647, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "v = 100\n",
    "NUM_GROUPS = 3\n",
    "NUM_CLASSES = 10\n",
    "tau = tfp.edward2.HalfNormal(scale=v*tf.ones([NUM_GROUPS]))\n",
    "print(tau[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 2, 1, 0, 2, 1, 0, 2, 0, 2)\n"
     ]
    }
   ],
   "source": [
    "LABELS_CHANGE_DICT_GROUPED = {0: 0, 3: 0, 6: 0, 8: 0,  # 0\n",
    "                              2: 1, 5: 1,  # 1\n",
    "                              1: 2, 4: 2, 7: 2, 9: 2}  # 2\n",
    "LABELS_CHANGE_GROUPED = []\n",
    "for i in range(NUM_CLASSES):\n",
    "    LABELS_CHANGE_GROUPED.append(LABELS_CHANGE_DICT_GROUPED[i])\n",
    "LABELS_CHANGE_GROUPED = tuple(LABELS_CHANGE_GROUPED)\n",
    "print(LABELS_CHANGE_GROUPED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "train_set, heldout_set = tf.keras.datasets.mnist.load_data(path='mnist.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Dependency imports\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import matplotlib\n",
    "\n",
    "from datetime import datetime\n",
    "#from time import time\n",
    "#from tensorboard.python.keras.callbacks import TensorBoard\n",
    "\n",
    "from matplotlib import figure  # pylint: disable=g-import-not-at-top\n",
    "from matplotlib.backends import backend_agg\n",
    "import numpy as np\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from utils import gpu_session\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "# TODO(b/78137893): Integration tests currently fail with seaborn imports.\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "try:\n",
    "    import seaborn as sns  # pylint: disable=g-import-not-at-top\n",
    "\n",
    "    HAS_SEABORN = True\n",
    "except ImportError:\n",
    "    HAS_SEABORN = False\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "\n",
    "model_type = \"dense_layer\"\n",
    "IMAGE_SHAPE = [28, 28, 1]\n",
    "NUM_TRAIN_EXAMPLES = 60000  # 60000\n",
    "NUM_HELDOUT_EXAMPLES = 10000  # 10000\n",
    "NUM_CLASSES = 10\n",
    "NUM_GROUPS = 3\n",
    "# Distribution of digits to groups\n",
    "LABELS_CHANGE_DICT_GROUPED = {0: 0, 3: 0, 6: 0, 8: 0,  # 0\n",
    "                              2: 1, 5: 1,  # 1\n",
    "                              1: 2, 4: 2, 7: 2, 9: 2}  # 2\n",
    "LABELS_CHANGE_GROUPED = []\n",
    "for i in range(NUM_CLASSES):\n",
    "    LABELS_CHANGE_GROUPED.append(LABELS_CHANGE_DICT_GROUPED[i])\n",
    "LABELS_CHANGE_GROUPED = tuple(LABELS_CHANGE_GROUPED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTSequence(tf.keras.utils.Sequence):\n",
    "    \"\"\"Produces a sequence of MNIST digits with labels.\"\"\"\n",
    "\n",
    "    def __init__(self, data=None, batch_size=128, used_labels=tuple(range(10)), labels_len=NUM_CLASSES,\n",
    "                 labels_change=tuple(range(10)), preprocessing=True, fake_data_size=None):\n",
    "        \"\"\"Initializes the sequence.\n",
    "\n",
    "    Args:\n",
    "      data: Tuple of numpy `array` instances, the first representing images and\n",
    "            the second labels.\n",
    "      batch_size: Integer, number of elements in each training batch.\n",
    "      fake_data_size: Optional integer number of fake datapoints to generate.\n",
    "    \"\"\"\n",
    "        if data:\n",
    "            images, labels = data\n",
    "        else:\n",
    "            images, labels = MNISTSequence.__generate_fake_data(\n",
    "                num_images=fake_data_size, num_classes=NUM_CLASSES)\n",
    "        if preprocessing:\n",
    "            self.images, self.labels = MNISTSequence.__preprocessing(\n",
    "                images, labels, used_labels, labels_change, labels_len)\n",
    "        else:\n",
    "            self.images, self.labels = images, labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    @staticmethod\n",
    "    def __generate_fake_data(num_images, num_classes):\n",
    "        \"\"\"Generates fake data in the shape of the MNIST dataset for unittest.\n",
    "\n",
    "    Args:\n",
    "      num_images: Integer, the number of fake images to be generated.\n",
    "      num_classes: Integer, the number of classes to be generate.\n",
    "    Returns:\n",
    "      images: Numpy `array` representing the fake image data. The\n",
    "              shape of the array will be (num_images, 28, 28).\n",
    "      labels: Numpy `array` of integers, where each entry will be\n",
    "              assigned a unique integer.\n",
    "    \"\"\"\n",
    "        images = np.random.randint(low=0, high=256,\n",
    "                                   size=(num_images, IMAGE_SHAPE[0],\n",
    "                                         IMAGE_SHAPE[1]))\n",
    "        labels = np.random.randint(low=0, high=num_classes,\n",
    "                                   size=num_images)\n",
    "        return images, labels\n",
    "\n",
    "    @staticmethod\n",
    "    def __preprocessing(images, labels, used_labels, labels_change, labels_len):\n",
    "        \"\"\"Preprocesses image and labels data.\n",
    "\n",
    "    Args:\n",
    "      images: Numpy `array` representing the image data.\n",
    "      labels: Numpy `array` representing the labels data (range 0-9).\n",
    "\n",
    "    Returns:\n",
    "      images: Numpy `array` representing the image data, normalized\n",
    "              and expanded for convolutional network input.\n",
    "      labels: Numpy `array` representing the labels data (range 0-9),\n",
    "              as one-hot (categorical) values.\n",
    "    \"\"\"\n",
    "        # Auxiliary dicts for integer label - its MNIST representation mapping\n",
    "        # labels_bin_dict = {i: tuple(np.identity(10)[i]) for i in range(10)}\n",
    "        # bin_labels_dict = {v: k for k, v in labels_bin_dict.items()}\n",
    "\n",
    "        # Get indices of used labels using convertation of MNIST labels from array to integer system\n",
    "        indices = []\n",
    "        for label in labels:\n",
    "            indices.append(int(label) in used_labels)\n",
    "        # Select used labels\n",
    "        labels_transformed = []\n",
    "        for index, flag in enumerate(indices):\n",
    "            if flag:\n",
    "                labels_transformed.append(labels_change[labels[index]])\n",
    "        # Normalize images\n",
    "        images = 2 * (images[indices] / 255.) - 1.\n",
    "        images = images[..., tf.newaxis]\n",
    "        # Convert labels for cross-entropy loss\n",
    "        labels = tf.keras.utils.to_categorical(y=labels_transformed, num_classes=labels_len)\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(tf.math.ceil(len(self.images) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.images[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = MNISTSequence(data=train_set, batch_size=128,\n",
    "            labels_change=LABELS_CHANGE_GROUPED, labels_len=NUM_GROUPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0.]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-318053497c5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_seq_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMNISTSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-1d4054c2b072>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, batch_size, used_labels, labels_len, labels_change, preprocessing, fake_data_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             images, labels = MNISTSequence.__generate_fake_data(\n\u001b[0;32m---> 18\u001b[0;31m                 num_images=fake_data_size, num_classes=NUM_CLASSES)\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             self.images, self.labels = MNISTSequence.__preprocessing(\n",
      "\u001b[0;32m<ipython-input-12-1d4054c2b072>\u001b[0m in \u001b[0;36m__generate_fake_data\u001b[0;34m(num_images, num_classes)\u001b[0m\n\u001b[1;32m     39\u001b[0m         images = np.random.randint(low=0, high=256,\n\u001b[1;32m     40\u001b[0m                                    size=(num_images, IMAGE_SHAPE[0],\n\u001b[0;32m---> 41\u001b[0;31m                                          IMAGE_SHAPE[1]))\n\u001b[0m\u001b[1;32m     42\u001b[0m         labels = np.random.randint(low=0, high=num_classes,\n\u001b[1;32m     43\u001b[0m                                    size=num_images)\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_bounded_integers.pyx\u001b[0m in \u001b[0;36mnumpy.random._bounded_integers._rand_int64\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2960\u001b[0m     \"\"\"\n\u001b[1;32m   2961\u001b[0m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[0;32m-> 2962\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "print(train_seq.labels[0])\n",
    "train_seq_1 = MNISTSequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [], []]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_accuracy, epoch_loss = [[] for i in range(NUM_GROUPS)], [[] for i in range(NUM_GROUPS)]\n",
    "epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Dependency imports\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import matplotlib\n",
    "\n",
    "from datetime import datetime\n",
    "# from time import time\n",
    "# from tensorboard.python.keras.callbacks import TensorBoard\n",
    "\n",
    "from matplotlib import figure  # pylint: disable=g-import-not-at-top\n",
    "from matplotlib.backends import backend_agg\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from utils import gpu_session\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "try:\n",
    "    import seaborn as sns  # pylint: disable=g-import-not-at-top\n",
    "\n",
    "    HAS_SEABORN = True\n",
    "except ImportError:\n",
    "    HAS_SEABORN = False\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "\n",
    "model_type = \"dense_layer\"\n",
    "IMAGE_SHAPE = [28, 28, 1]\n",
    "NUM_TRAIN_EXAMPLES = 60#000 # 60000\n",
    "NUM_HELDOUT_EXAMPLES = 10#000  # 10000\n",
    "NUM_CLASSES = 10\n",
    "NUM_GROUPS = 3\n",
    "# Distribution of digits to groups\n",
    "LABELS_CHANGE_DICT_GROUPED = {0: 0, 3: 0, 6: 0, 8: 0,  # 0\n",
    "                              2: 1, 5: 1,  # 1\n",
    "                              1: 2, 4: 2, 7: 2, 9: 2}  # 2\n",
    "LABELS_CHANGE_GROUPED = []\n",
    "for i in range(NUM_CLASSES):\n",
    "    LABELS_CHANGE_GROUPED.append(LABELS_CHANGE_DICT_GROUPED[i])\n",
    "LABELS_CHANGE_GROUPED = tuple(LABELS_CHANGE_GROUPED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/anton/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_probability/python/layers/util.py:106: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0504 11:31:13.816830 140400119461696 deprecation.py:323] From /home/anton/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_probability/python/layers/util.py:106: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "in converted code:\n\n    /home/anton/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_probability/python/layers/dense_variational.py:184 call\n        name='divergence_kernel')\n    /home/anton/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_probability/python/layers/dense_variational.py:306 _apply_divergence\n        posterior, prior, posterior_tensor),\n    <ipython-input-11-00b22f472ad6>:40 <lambda>\n        tf.cast(input_size, dtype=tf.float32))\n\n    NameError: name 'input_size' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-00b22f472ad6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m#bias_prior_fn=prior_0_distribution,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         activation=tf.nn.softmax)\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mw_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"W_0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m#w_0.compile(optimizer, loss='categorical_crossentropy',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    771\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    772\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: in converted code:\n\n    /home/anton/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_probability/python/layers/dense_variational.py:184 call\n        name='divergence_kernel')\n    /home/anton/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_probability/python/layers/dense_variational.py:306 _apply_divergence\n        posterior, prior, posterior_tensor),\n    <ipython-input-11-00b22f472ad6>:40 <lambda>\n        tf.cast(input_size, dtype=tf.float32))\n\n    NameError: name 'input_size' is not defined\n"
     ]
    }
   ],
   "source": [
    "output_size = 10\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)\n",
    "tau_0_inv = 1/1000.0\n",
    "v = 100.0\n",
    "models = []\n",
    "input = tfk.Input(shape=tuple(IMAGE_SHAPE), name='img')\n",
    "if 0:\n",
    "    for i in range(NUM_GROUPS):\n",
    "        l1 = tfp.layers.Convolution2DFlipout(\n",
    "            6, kernel_size=5, padding='SAME',\n",
    "            kernel_divergence_fn=kl_divergence_function,\n",
    "            activation=tf.nn.relu)(input)\n",
    "        x1 = l1(inputs)\n",
    "        l2 = tf.keras.layers.MaxPooling2D(\n",
    "            pool_size=[2, 2], strides=[2, 2],\n",
    "            padding='SAME')(l1)\n",
    "        l3 = tfp.layers.Convolution2DFlipout(\n",
    "            16, kernel_size=5, padding='SAME',\n",
    "            kernel_divergence_fn=kl_divergence_function,\n",
    "            activation=tf.nn.relu)(l2)\n",
    "        l4 = tf.keras.layers.MaxPooling2D(\n",
    "            pool_size=[2, 2], strides=[2, 2],\n",
    "            padding='SAME')(l3)\n",
    "        l5 = tfp.layers.Convolution2DFlipout(\n",
    "            120, kernel_size=5, padding='SAME',\n",
    "            kernel_divergence_fn=kl_divergence_function,\n",
    "            activation=tf.nn.relu)(l4)\n",
    "        l6 = tf.keras.layers.Flatten()(l5)\n",
    "        l7 = tfp.layers.DenseFlipout(\n",
    "            84, kernel_divergence_fn=kl_divergence_function,\n",
    "            activation=tf.nn.relu)(l6)\n",
    "        l8 = tfp.layers.DenseFlipout(\n",
    "            output_size, kernel_divergence_fn=kl_divergence_function,\n",
    "            activation=tf.nn.softmax)(l7)\n",
    "        models.append(tfk.Model(inputs=inputs, outputs=l8, name='dense_net_{}'.format(i)))\n",
    "        models[i].compile(optimizer, loss='categorical_crossentropy',\n",
    "                          metrics=['accuracy'], experimental_run_tf_function=False)\n",
    "if 1:\n",
    "    kl_divergence_function = (lambda q, p, _: tfd.kl_divergence(q, p) /  # pylint: disable=g-long-lambda\n",
    "                                              tf.cast(input_size, dtype=tf.float32))\n",
    "    f = tfkl.Flatten()(input)\n",
    "    prior_0_distribution = tfpl.util.default_mean_field_normal_fn(\n",
    "            loc_initializer=tf.zeros_initializer,\n",
    "            untransformed_scale_initializer=\n",
    "            tf.constant_initializer(tfp.math.softplus_inverse(tau_0_inv).numpy()))\n",
    "    x = tfpl.DenseFlipout(\n",
    "        output_size, kernel_divergence_fn=kl_divergence_function,\n",
    "        kernel_prior_fn=prior_0_distribution,\n",
    "        #bias_prior_fn=prior_0_distribution,\n",
    "        activation=tf.nn.softmax)\n",
    "    y = x(f)\n",
    "    w_0 = tfk.Model(input, y, name=\"W_0\")\n",
    "    #w_0.compile(optimizer, loss='categorical_crossentropy',\n",
    "    #                      metrics=['accuracy'], experimental_run_tf_function=False)\n",
    "    w = tf.Variable(x.get_weights()[0])\n",
    "    b = tf.Variable(x.get_weights()[1])\n",
    "    # Get group weights variances\n",
    "    tau_g_inv= tf.square(tfp.edward2.HalfNormal(scale=v * tf.ones([NUM_GROUPS])))\n",
    "\n",
    "    # Per group networks\n",
    "    models = []\n",
    "    denses = []\n",
    "    w0 = W0_weights(weights=x.get_weights()[0])\n",
    "    #b = w0.weights\n",
    "    inputs = []\n",
    "    flattens = []\n",
    "    outputs = []\n",
    "    for i in range(NUM_GROUPS):\n",
    "        inputs.append(tfk.Input(shape=tuple(IMAGE_SHAPE), name='img{}'.format(i)))\n",
    "        flattens.append(tfkl.Flatten()(inputs[i]))\n",
    "        a = x.get_weights()[0]\n",
    "        c = tf.constant_initializer(w.numpy())\n",
    "        d = tf.zeros\n",
    "        kl_divergence_function_g = (lambda q, p, _: tfd.kl_divergence(q, p) /\n",
    "                                                  tf.cast(input_size[i], dtype=tf.float32))\n",
    "        prior_g_kernel_distribution = tfpl.util.default_mean_field_normal_fn(\n",
    "                                loc_initializer=tf.constant_initializer(w.numpy()),\n",
    "                                untransformed_scale_initializer=\n",
    "                                tf.constant_initializer(tfp.math.softplus_inverse(tau_g_inv[i]).numpy()))\n",
    "        prior_g_bias_distribution = tfpl.util.default_mean_field_normal_fn(\n",
    "                                loc_initializer=tf.zeros,#constant_initializer(b.numpy()),\n",
    "                                untransformed_scale_initializer=\n",
    "                                tf.constant_initializer(tfp.math.softplus_inverse(tau_g_inv[i]).numpy()))\n",
    "        denses.append(tfpl.DenseFlipout(\n",
    "                        output_size,\n",
    "                        kernel_divergence_fn=kl_divergence_function_g,\n",
    "                        kernel_prior_fn=prior_g_kernel_distribution,\n",
    "                        #bias_prior_fn=prior_g_bias_distribution,\n",
    "                        activation=tf.nn.softmax))\n",
    "        outputs.append(denses[i](flattens[i]))\n",
    "        #models.append(tfk.Model(inputs=inputs, outputs=output, name=f'dense_net_{i}'))\n",
    "    model = tfk.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer, loss=['categorical_crossentropy'] * NUM_GROUPS,\n",
    "                  metrics=['accuracy'], experimental_run_tf_function=False)\n",
    "    tfk.utils.plot_model(model, to_file='multi_input_and_output_model.png', show_shapes=True)\n",
    "    if 0:\n",
    "        for i in range(NUM_GROUPS):\n",
    "            models[i].compile(optimizer, loss='categorical_crossentropy',\n",
    "                              metrics=['accuracy'], experimental_run_tf_function=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FATAL Flags parsing error: Unknown command line flag 'f'\n",
      "Pass --helpshort or --helpfull to see help on flags.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "E0504 11:13:27.580483 140400119461696 ultratb.py:152] Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None []\n",
      "\n",
      " []\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anton/anaconda3/envs/tf/lib/python3.7/site-packages/absl/app.py\", line 158, in parse_flags_with_usage\n",
      "    return FLAGS(args)\n",
      "  File \"/home/anton/anaconda3/envs/tf/lib/python3.7/site-packages/absl/flags/_flagvalues.py\", line 633, in __call__\n",
      "    name, value, suggestions=suggestions)\n",
      "absl.flags._exceptions.UnrecognizedFlagError: Unknown command line flag 'f'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anton/anaconda3/envs/tf/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-397719984269>\", line 514, in <module>\n",
      "    app.run(main)\n",
      "  File \"/home/anton/anaconda3/envs/tf/lib/python3.7/site-packages/absl/app.py\", line 293, in run\n",
      "    flags_parser,\n",
      "  File \"/home/anton/anaconda3/envs/tf/lib/python3.7/site-packages/absl/app.py\", line 362, in _run_init\n",
      "    flags_parser=flags_parser,\n",
      "  File \"/home/anton/anaconda3/envs/tf/lib/python3.7/site-packages/absl/app.py\", line 212, in _register_and_parse_flags_with_usage\n",
      "    args_to_main = flags_parser(original_argv)\n",
      "  File \"/home/anton/anaconda3/envs/tf/lib/python3.7/site-packages/absl/app.py\", line 162, in parse_flags_with_usage\n",
      "    sys.exit(1)\n",
      "SystemExit: 1\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anton/anaconda3/envs/tf/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/anton/anaconda3/envs/tf/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/anton/anaconda3/envs/tf/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/anton/anaconda3/envs/tf/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "AttributeError: 'tuple' object has no attribute 'tb_frame'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "# %load bayesian_neural_network.py\n",
    "# Copyright 2018 The TensorFlow Probability Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ============================================================================\n",
    "\"\"\"Trains a Bayesian neural network to classify MNIST digits.\n",
    "\n",
    "The architecture is LeNet-5 [1].\n",
    "\n",
    "#### References\n",
    "\n",
    "[1]: Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner.\n",
    "     Gradient-based learning applied to document recognition.\n",
    "     _Proceedings of the IEEE_, 1998.\n",
    "     http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Dependency imports\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import matplotlib\n",
    "\n",
    "from datetime import datetime\n",
    "# from time import time\n",
    "# from tensorboard.python.keras.callbacks import TensorBoard\n",
    "\n",
    "from matplotlib import figure  # pylint: disable=g-import-not-at-top\n",
    "from matplotlib.backends import backend_agg\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from utils import gpu_session\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "try:\n",
    "    import seaborn as sns  # pylint: disable=g-import-not-at-top\n",
    "\n",
    "    HAS_SEABORN = True\n",
    "except ImportError:\n",
    "    HAS_SEABORN = False\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "\n",
    "model_type = \"dense_layer\"\n",
    "IMAGE_SHAPE = [28, 28, 1]\n",
    "NUM_TRAIN_EXAMPLES = 60#000 # 60000\n",
    "NUM_HELDOUT_EXAMPLES = 10#000  # 10000\n",
    "NUM_CLASSES = 10\n",
    "NUM_GROUPS = 3\n",
    "# Distribution of digits to groups\n",
    "LABELS_CHANGE_DICT_GROUPED = {0: 0, 3: 0, 6: 0, 8: 0,  # 0\n",
    "                              2: 1, 5: 1,  # 1\n",
    "                              1: 2, 4: 2, 7: 2, 9: 2}  # 2\n",
    "LABELS_CHANGE_GROUPED = []\n",
    "for i in range(NUM_CLASSES):\n",
    "    LABELS_CHANGE_GROUPED.append(LABELS_CHANGE_DICT_GROUPED[i])\n",
    "LABELS_CHANGE_GROUPED = tuple(LABELS_CHANGE_GROUPED)\n",
    "\n",
    "flags.DEFINE_float('learning_rate',\n",
    "                   default=0.001,\n",
    "                   help='Initial learning rate.')\n",
    "flags.DEFINE_integer('num_epochs',\n",
    "                     default=10,\n",
    "                     help='Number of training steps to run.')\n",
    "flags.DEFINE_integer('num_grouped_epochs',\n",
    "                     default=10,\n",
    "                     help='Number of training steps to run grouped inference.')\n",
    "flags.DEFINE_integer('batch_size',\n",
    "                     default=128,\n",
    "                     help='Batch size.')\n",
    "flags.DEFINE_string('data_dir',\n",
    "                    default=os.path.join(os.getenv('TEST_TMPDIR', '/tmp'),\n",
    "                                         'bayesian_neural_network/data'),\n",
    "                    help='Directory where data is stored (if using real data).')\n",
    "flags.DEFINE_string(\n",
    "    'model_dir',\n",
    "    default=os.path.join(os.getenv('TEST_TMPDIR', '/tmp'),\n",
    "                         'bayesian_neural_network/'),\n",
    "    help=\"Directory to put the model's fit.\")\n",
    "integer = flags.DEFINE_integer('viz_steps', default=400, help='Frequency at which save visualizations.')\n",
    "flags.DEFINE_integer('num_monte_carlo',\n",
    "                     default=50,\n",
    "                     help='Network draws to compute predictive probabilities.')\n",
    "flags.DEFINE_bool('fake_data',\n",
    "                  default=False,\n",
    "                  help='If true, uses fake data. Defaults to real data.')\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "class W0_weights(object):\n",
    "    def __init__(self, weights=None):\n",
    "        self.weights = weights\n",
    "    def weights_init(self, shape, dtype=None):\n",
    "        return self.weights#tfk.backend.random_normal(shape, dtype=dtype)\n",
    "\n",
    "\n",
    "def create_model(model_type=\"LeNet\", input_size=(NUM_TRAIN_EXAMPLES,), output_size=NUM_CLASSES):\n",
    "    \"\"\"Creates a Keras model using the LeNet-5 architecture.\n",
    "\n",
    "  Returns:\n",
    "      model: Compiled Keras model.\n",
    "  \"\"\"\n",
    "    # KL divergence weighted by the number of training samples, using\n",
    "    # lambda function to pass as input to the kernel_divergence_fn on\n",
    "    # flipout layers.\n",
    "    #TODO: check correctness for per-group model\n",
    "\n",
    "\n",
    "    # Define a LeNet-5 model using three convolutional (with max pooling)\n",
    "    # and two fully connected dense layers. We use the Flipout\n",
    "    # Monte Carlo estimator for these layers, which enables lower variance\n",
    "    # stochastic gradients than naive reparameterization.\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=FLAGS.learning_rate)\n",
    "    # We use the categorical_crossentropy loss since the MNIST dataset contains\n",
    "    # ten labels. The Keras API will then automatically add the\n",
    "    # Kullback-Leibler divergence (contained on the individual layers of\n",
    "    # the model), to the cross entropy loss, effectively\n",
    "    # calcuating the (negated) Evidence Lower Bound Loss (ELBO)\n",
    "\n",
    "    if model_type == \"LeNet\":\n",
    "        kl_divergence_function = (lambda q, p, _: tfd.kl_divergence(q, p) /  # pylint: disable=g-long-lambda\n",
    "                                                  tf.cast(input_size, dtype=tf.float32))\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tfp.layers.Convolution2DFlipout(\n",
    "                6, kernel_size=5, padding='SAME',\n",
    "                kernel_divergence_fn=kl_divergence_function,\n",
    "                activation=tf.nn.relu),\n",
    "            tf.keras.layers.MaxPooling2D(\n",
    "                pool_size=[2, 2], strides=[2, 2],\n",
    "                padding='SAME'),\n",
    "            tfp.layers.Convolution2DFlipout(\n",
    "                16, kernel_size=5, padding='SAME',\n",
    "                kernel_divergence_fn=kl_divergence_function,\n",
    "                activation=tf.nn.relu),\n",
    "            tf.keras.layers.MaxPooling2D(\n",
    "                pool_size=[2, 2], strides=[2, 2],\n",
    "                padding='SAME'),\n",
    "            tfp.layers.Convolution2DFlipout(\n",
    "                120, kernel_size=5, padding='SAME',\n",
    "                kernel_divergence_fn=kl_divergence_function,\n",
    "                activation=tf.nn.relu),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tfp.layers.DenseFlipout(\n",
    "                84, kernel_divergence_fn=kl_divergence_function,\n",
    "                activation=tf.nn.relu),\n",
    "            tfp.layers.DenseFlipout(\n",
    "                output_size, kernel_divergence_fn=kl_divergence_function,\n",
    "                activation=tf.nn.softmax)\n",
    "        ])\n",
    "        model.compile(optimizer, loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'], experimental_run_tf_function=False)\n",
    "        return model\n",
    "    elif model_type == \"dense_layer\":\n",
    "        tau_0_inv = 1/1000.0\n",
    "        v = 100.0\n",
    "        models = []\n",
    "        input = tfk.Input(shape=tuple(IMAGE_SHAPE), name='img')\n",
    "        if 0:\n",
    "            for i in range(NUM_GROUPS):\n",
    "                l1 = tfp.layers.Convolution2DFlipout(\n",
    "                    6, kernel_size=5, padding='SAME',\n",
    "                    kernel_divergence_fn=kl_divergence_function,\n",
    "                    activation=tf.nn.relu)(input)\n",
    "                x1 = l1(inputs)\n",
    "                l2 = tf.keras.layers.MaxPooling2D(\n",
    "                    pool_size=[2, 2], strides=[2, 2],\n",
    "                    padding='SAME')(l1)\n",
    "                l3 = tfp.layers.Convolution2DFlipout(\n",
    "                    16, kernel_size=5, padding='SAME',\n",
    "                    kernel_divergence_fn=kl_divergence_function,\n",
    "                    activation=tf.nn.relu)(l2)\n",
    "                l4 = tf.keras.layers.MaxPooling2D(\n",
    "                    pool_size=[2, 2], strides=[2, 2],\n",
    "                    padding='SAME')(l3)\n",
    "                l5 = tfp.layers.Convolution2DFlipout(\n",
    "                    120, kernel_size=5, padding='SAME',\n",
    "                    kernel_divergence_fn=kl_divergence_function,\n",
    "                    activation=tf.nn.relu)(l4)\n",
    "                l6 = tf.keras.layers.Flatten()(l5)\n",
    "                l7 = tfp.layers.DenseFlipout(\n",
    "                    84, kernel_divergence_fn=kl_divergence_function,\n",
    "                    activation=tf.nn.relu)(l6)\n",
    "                l8 = tfp.layers.DenseFlipout(\n",
    "                    output_size, kernel_divergence_fn=kl_divergence_function,\n",
    "                    activation=tf.nn.softmax)(l7)\n",
    "                models.append(tfk.Model(inputs=inputs, outputs=l8, name='dense_net_{}'.format(i)))\n",
    "                models[i].compile(optimizer, loss='categorical_crossentropy',\n",
    "                                  metrics=['accuracy'], experimental_run_tf_function=False)\n",
    "        if 1:\n",
    "            kl_divergence_function = (lambda q, p, _: tfd.kl_divergence(q, p) /  # pylint: disable=g-long-lambda\n",
    "                                                      tf.cast(input_size, dtype=tf.float32))\n",
    "            f = tfkl.Flatten()(input)\n",
    "            prior_0_distribution = tfpl.util.default_mean_field_normal_fn(\n",
    "                    loc_initializer=tf.zeros_initializer,\n",
    "                    untransformed_scale_initializer=\n",
    "                    tf.constant_initializer(tfp.math.softplus_inverse(tau_0_inv).numpy()))\n",
    "            x = tfpl.DenseFlipout(\n",
    "                output_size, kernel_divergence_fn=kl_divergence_function,\n",
    "                kernel_prior_fn=prior_0_distribution,\n",
    "                #bias_prior_fn=prior_0_distribution,\n",
    "                activation=tf.nn.softmax)\n",
    "            y = x(f)\n",
    "            w_0 = tfk.Model(input, y, name=\"W_0\")\n",
    "            #w_0.compile(optimizer, loss='categorical_crossentropy',\n",
    "            #                      metrics=['accuracy'], experimental_run_tf_function=False)\n",
    "            w = tf.Variable(x.get_weights()[0])\n",
    "            b = tf.Variable(x.get_weights()[1])\n",
    "            # Get group weights variances\n",
    "            tau_g_inv= tf.square(tfp.edward2.HalfNormal(scale=v * tf.ones([NUM_GROUPS])))\n",
    "\n",
    "            # Per group networks\n",
    "            models = []\n",
    "            denses = []\n",
    "            w0 = W0_weights(weights=x.get_weights()[0])\n",
    "            #b = w0.weights\n",
    "            inputs = []\n",
    "            flattens = []\n",
    "            outputs = []\n",
    "            for i in range(NUM_GROUPS):\n",
    "                inputs.append(tfk.Input(shape=tuple(IMAGE_SHAPE), name='img{}'.format(i)))\n",
    "                flattens.append(tfkl.Flatten()(inputs[i]))\n",
    "                a = x.get_weights()[0]\n",
    "                c = tf.constant_initializer(w.numpy())\n",
    "                d = tf.zeros\n",
    "                kl_divergence_function_g = (lambda q, p, _: tfd.kl_divergence(q, p) /\n",
    "                                                          tf.cast(input_size[i], dtype=tf.float32))\n",
    "                prior_g_kernel_distribution = tfpl.util.default_mean_field_normal_fn(\n",
    "                                        loc_initializer=tf.constant_initializer(w.numpy()),\n",
    "                                        untransformed_scale_initializer=\n",
    "                                        tf.constant_initializer(tfp.math.softplus_inverse(tau_g_inv[i]).numpy()))\n",
    "                prior_g_bias_distribution = tfpl.util.default_mean_field_normal_fn(\n",
    "                                        loc_initializer=tf.zeros,#constant_initializer(b.numpy()),\n",
    "                                        untransformed_scale_initializer=\n",
    "                                        tf.constant_initializer(tfp.math.softplus_inverse(tau_g_inv[i]).numpy()))\n",
    "                denses.append(tfpl.DenseFlipout(\n",
    "                                output_size,\n",
    "                                kernel_divergence_fn=kl_divergence_function_g,\n",
    "                                kernel_prior_fn=prior_g_kernel_distribution,\n",
    "                                #bias_prior_fn=prior_g_bias_distribution,\n",
    "                                activation=tf.nn.softmax))\n",
    "                outputs.append(denses[i](flattens[i]))\n",
    "                #models.append(tfk.Model(inputs=inputs, outputs=output, name=f'dense_net_{i}'))\n",
    "            model = tfk.Model(inputs=inputs, outputs=outputs)\n",
    "            model.compile(optimizer, loss=['categorical_crossentropy'] * NUM_GROUPS,\n",
    "                          metrics=['accuracy'], experimental_run_tf_function=False)\n",
    "            tfk.utils.plot_model(model, to_file='multi_input_and_output_model.png', show_shapes=True)\n",
    "            if 0:\n",
    "                for i in range(NUM_GROUPS):\n",
    "                    models[i].compile(optimizer, loss='categorical_crossentropy',\n",
    "                                      metrics=['accuracy'], experimental_run_tf_function=False)\n",
    "        return models, w_0, w\n",
    "\n",
    "\n",
    "class MNISTSequence(tf.keras.utils.Sequence):\n",
    "    \"\"\"Produces a sequence of MNIST digits with labels.\"\"\"\n",
    "\n",
    "    def __init__(self, data=None, batch_size=128, used_labels=tuple(range(10)), labels_len=NUM_CLASSES,\n",
    "                 labels_change=tuple(range(10)), preprocessing=True, fake_data_size=None):\n",
    "        \"\"\"Initializes the sequence.\n",
    "\n",
    "    Args:\n",
    "      data: Tuple of numpy `array` instances, the first representing images and\n",
    "            the second labels.\n",
    "      batch_size: Integer, number of elements in each training batch.\n",
    "      fake_data_size: Optional integer number of fake datapoints to generate.\n",
    "    \"\"\"\n",
    "        if data:\n",
    "            images, labels = data\n",
    "        else:\n",
    "            images, labels = MNISTSequence.__generate_fake_data(\n",
    "                num_images=fake_data_size, num_classes=NUM_CLASSES)\n",
    "        if preprocessing:\n",
    "            self.images, self.labels = MNISTSequence.__preprocessing(\n",
    "                images, labels, used_labels, labels_change, labels_len)\n",
    "        else:\n",
    "            self.images, self.labels = images, labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    @staticmethod\n",
    "    def __generate_fake_data(num_images, num_classes):\n",
    "        \"\"\"Generates fake data in the shape of the MNIST dataset for unittest.\n",
    "\n",
    "    Args:\n",
    "      num_images: Integer, the number of fake images to be generated.\n",
    "      num_classes: Integer, the number of classes to be generate.\n",
    "    Returns:\n",
    "      images: Numpy `array` representing the fake image data. The\n",
    "              shape of the array will be (num_images, 28, 28).\n",
    "      labels: Numpy `array` of integers, where each entry will be\n",
    "              assigned a unique integer.\n",
    "    \"\"\"\n",
    "        images = np.random.randint(low=0, high=256,\n",
    "                                   size=(num_images, IMAGE_SHAPE[0],\n",
    "                                         IMAGE_SHAPE[1]))\n",
    "        labels = np.random.randint(low=0, high=num_classes,\n",
    "                                   size=num_images)\n",
    "        return images, labels\n",
    "\n",
    "    @staticmethod\n",
    "    def __preprocessing(images, labels, used_labels, labels_change, labels_len):\n",
    "        \"\"\"Preprocesses image and labels data.\n",
    "\n",
    "    Args:\n",
    "      images: Numpy `array` representing the image data.\n",
    "      labels: Numpy `array` representing the labels data (range 0-9).\n",
    "\n",
    "    Returns:\n",
    "      images: Numpy `array` representing the image data, normalized\n",
    "              and expanded for convolutional network input.\n",
    "      labels: Numpy `array` representing the labels data (range 0-9),\n",
    "              as one-hot (categorical) values.\n",
    "    \"\"\"\n",
    "        # Auxiliary dicts for integer label - its MNIST representation mapping\n",
    "        # labels_bin_dict = {i: tuple(np.identity(10)[i]) for i in range(10)}\n",
    "        # bin_labels_dict = {v: k for k, v in labels_bin_dict.items()}\n",
    "\n",
    "        # Get indices of used labels using convertation of MNIST labels from array to integer system\n",
    "        indices = []\n",
    "        for label in labels:\n",
    "            indices.append(int(label) in used_labels)\n",
    "        # Select used labels\n",
    "        labels_transformed = []\n",
    "        for index, flag in enumerate(indices):\n",
    "            if flag:\n",
    "                labels_transformed.append(labels_change[labels[index]])\n",
    "        # Normalize images\n",
    "        images = 2 * (images[indices] / 255.) - 1.\n",
    "        images = images[..., tf.newaxis]\n",
    "        # Convert labels for cross-entropy loss\n",
    "        labels = tf.keras.utils.to_categorical(y=labels_transformed, num_classes=labels_len)\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(tf.math.ceil(len(self.images) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.images[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "\n",
    "def train_model(model, train_seq, heldout_seq=((), ()), tensorboard_callback=None):\n",
    "    \"\"\"\n",
    "    Trains LeNet model on MNIST data in a flexible to data way\n",
    "\n",
    "    :param model:\n",
    "    :param train_seq:\n",
    "    :param heldout_seq:\n",
    "    :param tensorboard_callback:\n",
    "    :return: trained model\n",
    "    \"\"\"\n",
    "\n",
    "    print(' ... Training neural network')\n",
    "    for epoch in range(FLAGS.num_epochs):\n",
    "        print('Epoch {}'.format(epoch))\n",
    "        epoch_accuracy, epoch_loss = [], []\n",
    "        for step, (batch_x, batch_y) in enumerate(train_seq):\n",
    "            batch_loss, batch_accuracy = model.train_on_batch(\n",
    "                batch_x, batch_y)\n",
    "            epoch_accuracy.append(batch_accuracy)\n",
    "            epoch_loss.append(batch_loss)\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print('Epoch: {}, Batch index: {}, '\n",
    "                      'Loss: {:.3f}, Accuracy: {:.3f}'.format(\n",
    "                    epoch, step,\n",
    "                    tf.reduce_mean(epoch_loss),\n",
    "                    tf.reduce_mean(epoch_accuracy)))\n",
    "    if 0:\n",
    "        training_history = model1.fit(\n",
    "            train_seq.images,  # input\n",
    "            train_seq.labels,  # output\n",
    "            batch_size=FLAGS.batch_size,\n",
    "            verbose=1,  # Suppress chatty output; use Tensorboard instead\n",
    "            epochs=FLAGS.num_epochs,\n",
    "            # validation_data=(x_test, y_test),\n",
    "            callbacks=[tensorboard_callback],\n",
    "        )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_labels_groups(labels_change_dict_grouped):\n",
    "    \"\"\"\n",
    "    :param labels_change_dict_grouped: dict {class: group}\n",
    "    :return: NUM_GROUPS-length tuple of tuples of corresp. labels\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for val in range(NUM_GROUPS):\n",
    "        res_aux = []\n",
    "        for key, value in labels_change_dict_grouped.items():\n",
    "            if value == val:\n",
    "                res_aux.append(key)\n",
    "        res.append(tuple(res_aux))\n",
    "    return tuple(res)\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    logdir = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "    train_set, heldout_set = tf.keras.datasets.mnist.load_data(path='mnist.npz')\n",
    "\n",
    "    # Cut sets if needed for optimization\n",
    "    train_set = (train_set[0][0:NUM_TRAIN_EXAMPLES], train_set[1][0:NUM_TRAIN_EXAMPLES])\n",
    "    heldout_set = (heldout_set[0][0:NUM_HELDOUT_EXAMPLES], heldout_set[1][0:NUM_HELDOUT_EXAMPLES])\n",
    "\n",
    "    # NUM_GROUPS-length tuple of tuples of corresp. labels, e.g. ((0,3,6,8), (2,5), (1,4,7,9))\n",
    "    labels_groups = get_labels_groups(LABELS_CHANGE_DICT_GROUPED)\n",
    "    # Training grouped model (for prediciton) on the whole data but with merged classes\n",
    "    train_seq_grouped = MNISTSequence(data=train_set, batch_size=FLAGS.batch_size,\n",
    "                                      labels_change=LABELS_CHANGE_GROUPED, labels_len=NUM_GROUPS)\n",
    "    model_grouped = create_model(model_type=\"LeNet\",\n",
    "                                 input_size=np.shape(train_seq_grouped.labels)[0],\n",
    "                                 output_size=NUM_GROUPS)\n",
    "    model_grouped = train_model(model_grouped, train_seq_grouped, )\n",
    "\n",
    "    # Creating hierarchical model\n",
    "    train_seqs = []\n",
    "    for i in range(NUM_GROUPS):\n",
    "        train_seqs.append(MNISTSequence(data=train_set, batch_size=FLAGS.batch_size,\n",
    "                                        used_labels=labels_groups[i],\n",
    "                                        labels_len=NUM_CLASSES,\n",
    "                                        labels_change=tuple(range(10)), ))\n",
    "    models, w_0, w = create_model(model_type=\"dense_layer\",\n",
    "                      input_size=[np.shape(train_seq_now.labels)[0] for train_seq_now in train_seqs],\n",
    "                      output_size=NUM_CLASSES)\n",
    "    print(w_0.get_weights()[0][0])\n",
    "\n",
    "    # Training the hierarchical model\n",
    "    print(' ... Training neural network')\n",
    "    for epoch in range(FLAGS.num_epochs):\n",
    "        print('Epoch {}'.format(epoch))\n",
    "        epoch_accuracy, epoch_loss = [], []\n",
    "        for i in np.random.permutation(tuple(range(NUM_GROUPS))):\n",
    "            #print(w_0.get_weights()[0][0,0])\n",
    "            #print(w.numpy()[0][0])\n",
    "            for step, (batch_x, batch_y) in enumerate(train_seqs[i]):\n",
    "                batch_loss, batch_accuracy = models[i].train_on_batch(\n",
    "                    batch_x, batch_y)\n",
    "                epoch_accuracy.append(batch_accuracy)\n",
    "                epoch_loss.append(batch_loss)\n",
    "                #print(models[0].get_weights()[0][0][0])\n",
    "                #print(models[1].get_weights()[0][0][0])\n",
    "                #print(models[2].get_weights()[0][0][0])\n",
    "                #print(w.numpy()[0][0])\n",
    "\n",
    "                if step % 100 == 0:\n",
    "                    print('Epoch: {}, Batch index: {}, '\n",
    "                          'Loss: {:.3f}, Accuracy: {:.3f}'.format(\n",
    "                        epoch, step,\n",
    "                        tf.reduce_mean(epoch_loss),\n",
    "                        tf.reduce_mean(epoch_accuracy)))\n",
    "    # Test\n",
    "    heldout_seq_grouped = MNISTSequence(data=heldout_set, batch_size=FLAGS.batch_size,\n",
    "                                        labels_change=LABELS_CHANGE_GROUPED, labels_len=NUM_GROUPS)\n",
    "    heldout_seq_classes = MNISTSequence(data=heldout_set, batch_size=FLAGS.batch_size, labels_len=NUM_CLASSES)\n",
    "\n",
    "    # Predict groups\n",
    "    predicted_groups_probs = model_grouped.predict(x=heldout_seq_grouped.images, batch_size=None, verbose=1)\n",
    "    print(\"Got predicted groups\")\n",
    "\n",
    "    # Depending on group predict class\n",
    "    predictions_right = []\n",
    "   # for i in range(np.shape(heldout_seq_grouped.labels)[0]):\n",
    "    # Get corresponding to group data\n",
    "    res = np.zeros((np.shape(heldout_seq_grouped.labels)[0], NUM_CLASSES))\n",
    "    for j in range(NUM_GROUPS):\n",
    "        predicted = models[j].predict(x=heldout_seq_classes.images, batch_size=None, verbose=1)\n",
    "        a = np.matlib.repmat(predicted_groups_probs[:, j], NUM_CLASSES, 1)\n",
    "        res += np.multiply(a.transpose(), predicted)\n",
    "    labels_bin_dict = np.identity(10)\n",
    "    for i in range(np.shape(heldout_seq_grouped.labels)[0]):\n",
    "        pos = np.where(res[i, :] == np.amax(res[i, :]))[0]\n",
    "        print(\"{}-{}\".format(labels_bin_dict[pos][0], heldout_seq_classes.labels[i]))\n",
    "        predictions_right.append(np.array_equal(\n",
    "                                labels_bin_dict[pos][0], heldout_seq_classes.labels[i])\n",
    "                                )\n",
    "    true_all = np.count_nonzero(predictions_right)\n",
    "    number_all = len(predictions_right)\n",
    "    print('Final results: {}/{} = {}'.format(true_all, number_all, true_all / number_all))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gpu_session(num_gpus=1)\n",
    "    app.run(main)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
